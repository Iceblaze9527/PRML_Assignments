# 第五次作业_2
> 新雅62/CDIE6 
> 2016013327 项雨桐

## 6.4 计算机小实验 1:用 KNN 进行人脸识别
在之前的作业中，我们已经用 softmax regression、多层感知器以及 SVM 设 计了人脸分类器，在本题中，请使用 KNN 进行同样的实验。并且请在十分类问题中，继续讨论以下问题:

1.  探讨不同的k值对分类器性能的影响: 分别取``k=1,3,5,7,9``，观察测试正确率随 k 值的变化。
2.  探讨不同的距离度量方式对分类器性能的影响: 选取你认为最优的 k 值，分别以欧氏距离、曼哈顿距离、切比雪夫距离、余弦距离作为距离度量，观察距离度量方式对测试正确率的影响。 

### 6.4.1 默认值（``n_neighbors=5, metric="euclidean_distance"``）下与其他方式的比较
```
测试准确率: 
2分类: 0.8467 
5分类: 0.7227 
10分类: 0.548 
2分类性能参数: 
TPR: 0.6857 
FPR: 0.0125 
TNR: 0.9875 
FNR: 0.3143 
sensitivity: 0.6857 
specificity: 0.9875 
FDR: 0.0204
AUC: 0.9501
```
![ROC_knn.png](https://i.loli.net/2020/03/24/oQXkaPMcfjwlzL3.png)
#### 特性
-   随分类个数的增加，KNN的准确率降低
-   KNN是非线性分类器
-  在默认参数下，KNN的准确率普遍比其他方式低，特别是在多分类问题的情况下
-   相对于其他模型，KNN训练较快，超参数较少；但预测速度较慢

### 6.4.2 k值对准确率的影响
``k=1,3,5,7,9`` 时模型的测试准确率如下

```
k=1: 0.58 
k=3: 0.548 
k=5: 0.548 
k=7: 0.5493333333333333
k=9: 0.5373333333333333
```
由此可见，随着k值的增大，模型的测试准确率总体呈下降趋势，预测准确率最高的k值为1。这说明样本中噪声数据对预测结果的影响小于远距离点的影响，模型倾向于欠拟合。

### 6.4.3 距离度量方式对准确率的影响
在``k=1``的基础上改变度量方式，得到预测准确率如下：

```
euclidean: 0.5506666666666666 
manhattan: 0.604 
chebyshev: 0.24533333333333332 
cosine: 0.6053333333333333
```

由此可见，度量方式对预测准确率的影响显著。其中，前三种距离都属于闵可夫斯基距离，p值分别为2，1，$\infty$，分类准确率随p值的增加而减小；而三种距离在数值上满足$L_{\mathrm{man}}\geqslant L_{\mathrm{euc}}\geqslant L_{\mathrm{che}}$，这说明样本点数据之间的距离需要被放大才有更好的预测效果。另外，余弦距离在准确率上取得了与曼哈顿距离相似的效果。


## 6.5 计算机小实验 2:用随机森林进行人脸识别
请使用随机森林进行同样的人脸识别实验，并且在十分类问题中，继续讨论以下问题:

1.  探讨树的数量对分类器性能的影响: 改变随机森林中决策树的数量，观察并比较训练正确率与测试正确率的变化，说明产生这种现象的原因。
2.  探讨树的最大深度对分类器性能的影响: 选取你认为最优的决策树数量 ，调整参数限制随机森林中决策树的最大深度，观察测试正确率随决策树最大深度的变化。


### 6.5.1 默认值（``n_estimator=100, max_depth=None``）下与其他方式的比较
```
测试准确率: 
2分类: 0.7867 
5分类: 0.712 
10分类: 0.5693 
2分类性能参数: 
TPR: 0.775 
FPR: 0.2 
TNR: 0.8 
FNR: 0.225 
sensitivity: 0.775 
specificity: 0.8 
FDR: 0.1842
AUC: 0.8878
```
![ROC_rfc.png](https://i.loli.net/2020/03/24/XAk2yDf5TYHl6jU.png)
#### 特性
-   随分类个数的增加，随机森林的准确率降低
-  在默认参数下，随机森林的准确率普遍比其他方式低
-   相对于其他模型，随机森林属于集成学习方法，训练速度较慢，超参数较多


### 6.5.2 树的个数对准确率的影响
设置树的数量 ``n=25,50,100,250,500,1000`` 得到的结果如下：
```
训练准确率: 
n=25: 1.0 
n=50: 1.0 
n=100: 1.0 
n=250: 1.0 
n=500: 1.0 
n=1000: 1.0 
测试准确率: 
n=25: 0.5293333333333333 
n=50: 0.5666666666666667 
n=100: 0.5693333333333334 
n=250: 0.5906666666666667 
n=500: 0.6053333333333333 
n=1000: 0.612
```
由此可见，随机森林在训练集上的准确率均可达到100%，但是在测试集上的准确率比较小，且随着树个数的减少而减少。这说明由于其对样本做彻底分类的特性，随机森林方法中的决策树单元易产生过拟合现象；而将多个分类器集成在一起可以提高准确率。

### 6.5.3 树的最大深度对准确率的影响
选择最优决策树数量``n=1000``，限制决策树最大深度分别为``d=5, 10, 20, 50, 100`` 得到结果如下：

```
d=5: 0.444 
d=10: 0.5826666666666667 
d=20: 0.6186666666666667 
d=50: 0.6173333333333333 
d=100: 0.6133333333333333
```

从中可以看出，将决策树的最大深度限制在合适范围内可以提高随机森林的预测准确率。在``d=5,10``时，由于层数过少，出现欠拟合现象; 而当d逐渐变大时，模型逐步倾向过拟合。本次实验中理想的最大深度``d=20``，而允许``d=50, 100``时预测准确率基本不变或略有减小，因此限制最大深度还可以在保证准确率的基础上减小计算开销。
<!--stackedit_data:
eyJoaXN0b3J5IjpbODk4MzUxNDEyXX0=
-->