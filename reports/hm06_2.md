# 第六次作业_2

> 新雅62/CDIE6
> 2016013327 项雨桐

## 7.5 计算机小实验 1:最大似然估计与贝叶斯估计
现有样本数据集$\boldsymbol{X}=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}$，我们假设其来自于正态分布$N\left(\mu, \sigma^{2}\right)$，请分别推导最大似然估计与贝叶斯估计(推导贝叶斯估计时假定方差已知)的模型参数，并完成以下问题: 

1.  请从标准正态分布𝑁(0,1)中分别抽取 10，100，1000 个样本数据作为𝑿，利用**最大似然估计**正态分布假设下的模型参数，分别重复三次实验，将同一抽样量下的三次重复实验估计的概率密度分布曲线绘制在一张图片内，并与标准正态分布的概率密度分布曲线比较。
 > 单变量正态分布：
 > $$p(x | \mu, \sigma^2)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right]
$$在每个样本点处的对数似然：$$\frac{\partial}{\partial \mu} \log{p(x_i | \mu, \sigma^2)} = \frac{1}{\sigma^2} (x_i-\mu)\\\frac{\partial}{\partial \sigma^2} \log{p(x_i | \mu, \sigma^2)} = -\frac{1}{2\sigma^2} + \frac{1}{2\sigma^4} (x_i-\mu)^2$$$$\sum_{i=1}^{n}\frac{\partial}{\partial \mu} \log{p(x_i | \mu, \sigma^2)} = 0 \Rightarrow \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i \\ \sum_{i=1}^{n} \frac{\partial}{\partial \sigma^2} \log{p(x_i | \mu, \sigma^2)} = 0 \Rightarrow \hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^{n} (x_i-\mu)^2$$因此利用最大似然估计得到的概率分布服从 $\mathcal{N}(\hat{\mu}, \hat{\sigma^2})$
>
>三次重复抽样后利用``matplotlib``绘制曲线如下：
```
1. N=10
mu11=-0.1147 sig11=1.0864
mu12=-0.2159 sig12=0.7489
mu13=-0.0971 sig13=1.1909
```
![norm10.png](https://i.loli.net/2020/03/30/Ix9GEFATMrsidHR.png)
```
2. N=100:
mu21=-0.1104 sig21=1.0629
mu22=-0.1465 sig22=1.0213
mu23=0.0606 sig23=0.8852
```

![norm100.png](https://i.loli.net/2020/03/30/t5FleAs89YhyOnQ.png)
```
3. N=1000:
mu31=0.0183 sig31=0.9821
mu32=-0.0088 sig32=1.0103
mu33=0.0388 sig33=0.9810
```

![norm1000.png](https://i.loli.net/2020/03/30/P5nbZMJGToUpAj2.png)
>
> 由此可见，随着样本数目的增多，最大似然估计逐渐接近样本的理论分布。

2.  假设均值𝜇满足正态先验$p(\mu) \sim N\left(-5, \sigma_{0}^{2}\right)$，请利用(1)中样本容量为1000的样本集𝑿，绘制出当$\sigma_{0}^{2}=0.01 \sigma^{2}, 0.1 \sigma^{2}, \sigma^{2}, 10 \sigma^{2}$时**贝叶斯**估计的概率密度函数曲线，并与标准正态分布进行比较。
    
> 由贝叶斯估计的理论，可以得知$$
\sigma_{N}^{2}=\frac{\sigma_{0}^{2} \sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}}, \quad \hat{\mu}=\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} m_{N}+\frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}$$其中$m_{N}=\frac{1}{N} \sum_{i=1}^{N} \boldsymbol{x}_{i}$。本实验中$N=1000$，$\sigma^2 = 1$，整理可得：$$
\sigma_{N}^{2}=\frac{1}{1000 +\frac{\sigma^{2}}{\sigma_{0}^{2}}}, \quad \hat{\mu}=\frac{1000}{1000 +\frac{\sigma^{2}}{\sigma_{0}^{2}}} m_{N}+\frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{1000 +\frac{\sigma^{2}}{\sigma_{0}^{2}}} (-5)$$实验中$\frac{\sigma^{2}}{\sigma_{0}^{2}} = 100,10,1,0.1$，可得均值的估计分布：
```
mu1=-0.4379 sig1=0.0009
mu2=-0.0313 sig2=0.0010
mu3=0.0133 sig3=0.0010
mu4=0.0178 sig4=0.0010
```
> 对应的曲线图如下：
![norm_bayes.png](https://i.loli.net/2020/03/30/XxThUidjS5tYM2C.png)
>由此可见$\sigma_{0}^{2}$越小，均值估计的误差越大；当$\sigma_{0}^{2}$增大到一定程度时，就已经可以得到较好的估计。

3.  改从均匀分布$U(0,1)$中抽取 100 个样本数据作为𝑿，正态分布的假设不变，重复(1)的实验，绘制出估计得到的概率密度分布曲线图，并与均匀分布$U(0,1)$的概率密度分布曲线图比较。

> 正态分布假设下的估计结果表达式不变，在一致分布的样本中得到的参数最大似然估计如下：
```
MLE with N=100:
5. muU1=0.4806 sigU1=0.2843
6. muU2=0.4486 sigU2=0.2738
7. muU3=0.4859 sigU3=0.2944
```
> 对应的曲线图如下：
> ![uniform.png](https://i.loli.net/2020/03/30/UDM5vfZTp6otnRS.png)
> 
> 由图可见，对于在正态分布假设下预测一致分布的样本得到的概率密度分布和对应的一致分布相比，二者在对应点概率密度值有较大差异，但在概率质量上比较接近。说明模型的选择对预测结果的表征有一定影响，但也能在一些指标上体现样本的特性。

4. 通过上述实验，讨论模型的选择、样本量以及先验分布对参数估计的影响。
> 在参数估计的框架中，样本实际分布模型的选择是首要的，也是最困难的。模型形式越接近实际，得到的预测效果就越好；反之，如果模型的选择不符合实际，则得不到最满意的结果。
> 对于参数估计方法，样本量同样参与决定了准确度。一般而言，样本量越大，得到的估计结果就越准确。
> 在贝叶斯估计中，预测结果受先验分布和样本（经验）的共同作用。一个好的先验可以在较少样本的情况下得到准确的预测结果；反之，一个不良好的先验需要更多的样本才能得到准确估计。对于实验2，在一定数目样本的情形下，对于一个有较大偏差的先验，只有给先验同时提供较大的不确定度（方差），才有可能在经验的修正下得到准确的预测；换言之，先验的确定程度越高，经验能够提供的修正信息就越少。

## 7.6 计算机小实验 2:非参数估计与贝叶斯决策  
请生成 500 个样本数据，其中 250 个数据点采样于$N(-2.5,1)$，记为正样本；剩余 250 个数据采样于$N(2.5,2)$，记为负样本。随机取出 70%的数据作为训练集，30%的数据作为测试集，完成以下问题:

1.  利用 Parzen 窗的高斯核，使用训练集中的数据对正样本和负样本分别进行非参数估计。
> 估计的概率密度分布如下：
> ![non_param.png](https://i.loli.net/2020/03/30/Ck8nFae6ZbliuMw.png)

2.  利用(1)中非参数估计的概率密度，请使用最小错误率的贝叶斯决策对测试集样本进行预测，给出测试集的错误率。
> 由贝叶斯公式$$
P\left(\omega_{i} | x\right)=\frac{p\left(x | \omega_{i}\right) P\left(\omega_{i}\right)}{p(x)}=\frac{p\left(x | \omega_{i}\right) P\left(\omega_{i}\right)}{\sum_{j=1}^{2} p\left(x | \omega_{j}\right) P\left(\omega_{j}\right)}
$$并且$P\left(\omega_{1}\right) = P\left(\omega_{2}\right) = \frac{1}{2}$因此可得：$$
P\left(\omega_{1} | x\right)=\frac{p\left(x | \omega_{1}\right) }{p\left(x | \omega_{1}\right) +p\left(x | \omega_{2}\right)}\\P\left(\omega_{2} | x\right)=\frac{p\left(x | \omega_{2}\right) }{p\left(x | \omega_{1}\right) +p\left(x | \omega_{2}\right)}
$$依最小错误率贝叶斯决策$$\text { if } \quad P\left(\omega_{1} | x\right)\begin{array}{c}> \\<
\end{array}P\left(\omega_{2} | x\right), \text { assign } \quad \begin{array}{c}x \in \omega_{1} \\x \in \omega_{2}
\end{array}
$$得到的测试集准确率为0.94
    
3.  利用(1)中非参数估计的概率密度，请使用最小风险的贝叶斯决策对测试集样本进行预测，惩罚矩阵如下:

真实值\预测值 |正样本|负样本
-- | -- |--
正样本 | 0 | 10
负样本 | 1 | 0

> 依最小风险贝叶斯决策$$\text { if } \quad \lambda_{11} P\left(\omega_{1} | x\right)+\lambda_{12} P\left(\omega_{2} | x\right)\begin{array}{c}> \\<
\end{array}\lambda_{21} P\left(\omega_{1} | x\right)+\lambda_{22} P\left(\omega_{2} | x\right), \text { assign } \begin{array}{l}
x \in \omega_{1} \\
x \in \omega_{2}
\end{array}
$$又 $\lambda_{12} = 10$，$\lambda_{21} = 1$，可得测试集准确率为0.8667

4. 结合你的实验结果，简述最小风险准则与最小错误率准则有什么不同。
> 最小错误率准则只关心将决策错误率降到最小，而最小风险准则关心的是做出错误决策的损失最小；错误率的最小可以转化为后验概率最大化，而损失的最小可以转化成经损失函数加权后的后验概率最大，在这个意义下，最小错误率是错分情况损失权重为1的最小风险决策的特例。
> 在本例中可以看出，仅论分类准确率，最小错误率准则要比最小风险准则要高很多，但因为我们认为做出假正例的判断的风险比假负例的判断高很多，所以模型更倾向于将样本判断为负例，以求风险最小化。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3ODkxOTgxNl19
-->