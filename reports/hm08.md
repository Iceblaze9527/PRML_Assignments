# 第八次作业
> 新雅62/CDIE6
> 2016013327 项雨桐

## 10.1 
在本题中，我们将通过理论计算推导线性回归模型的偏差与方差。考虑如 下线性回归模型:
$$
y_{i}=\boldsymbol{x}_{i}^{T} \boldsymbol{\beta}^{*}+\epsilon_{i}, \quad i=1, \ldots, n
$$
其中$\boldsymbol{x}_{\mathbf{1}}, \ldots, \boldsymbol{x}_{\boldsymbol{n}} \in \mathbb{R}^{p}$是确定的样本值，$\boldsymbol{\beta}^{*} \in \mathbb{R}^{p}$是未知的系数向量，$\epsilon_{1}, \dots, \epsilon_{n}$是误差，它们独立同 $\mathcal{N}\left(0, \sigma^{2}\right)$ 分布。记 $\boldsymbol{x}=\left(\boldsymbol{x}_{\mathbf{1}}^{\boldsymbol{T}} ; \ldots ; \boldsymbol{x}_{\boldsymbol{n}}^{\boldsymbol{T}}\right) \in \mathbb{R}^{n \times \boldsymbol{p}}, \boldsymbol{x}$的每一列互相独立，$\boldsymbol{y}=\left(y_{1}, \ldots, y_{n}\right)^{T} \in \mathbb{R}^{n}$为输出向量。利用最小二乘法可拟合得到系数的估计:$$\widehat{\boldsymbol{\beta}}=\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{y}$$同时，记模型在任意样本点$\boldsymbol{x_0}$处的回归输出为$\hat{f}\left(\boldsymbol{x}_{0}\right)=\boldsymbol{x}_{0}^{T} \widehat{\boldsymbol{\beta}}$。

1. 回忆模型偏差的定义，在任意样本点$\boldsymbol{x_0}$处，模型的偏差为: $$
\operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{0}\right)\right)=\mathbb{E}\left[\hat{f}\left(\boldsymbol{x}_{0}\right)\right]-y\left(\boldsymbol{x}_{0}\right)
$$其中$y\left(\boldsymbol{x_{0}}\right)=\boldsymbol{x_{0}}^{T} \boldsymbol{\beta}^{*}$为真实回归值。请证明$\operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{0}\right)\right)=0$，从而所有样本线性回归的平均偏差也为 0，即：$$
\frac{1}{n} \sum_{i=1}^{n} \operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right)=0
$$

> 答：$$
\operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{0}\right)\right)=\mathbb{E}\left[\hat{f}\left(\boldsymbol{x}_{0}\right)\right]-y\left(\boldsymbol{x}_{0}\right) = \mathbb{E}\left[\boldsymbol{x}_{0}^{T} \widehat{\boldsymbol{\beta}}\right] - \boldsymbol{x_{0}}^{T} \boldsymbol{\beta}^{*} = \boldsymbol{x_{0}}^{T}\left(\mathbb{E}\left[\widehat{\boldsymbol{\beta}} \right] - \boldsymbol{\beta}^{*} \right)
$$则$$\operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{0}\right)\right)=0 \Leftrightarrow \mathbb{E}\left[\widehat{\boldsymbol{\beta}} \right] = \boldsymbol{\beta}^{*} $$而$$\widehat{\boldsymbol{\beta}}=\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{y}$$$$
y_{i}=\boldsymbol{x}_{i}^{T} \boldsymbol{\beta}^{*}+\epsilon_{i}, \quad i=1, \ldots, n
$$转换成向量形式$$\boldsymbol{y} = \boldsymbol{x}\boldsymbol{\beta}^{*}+\boldsymbol{\epsilon}$$其中$$\boldsymbol{\epsilon} = (\epsilon_1, \epsilon_2, \ldots, \epsilon_n)^T$$代入得$$\widehat{\boldsymbol{\beta}} = \left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{\boldsymbol{T}}  (\boldsymbol{x}\boldsymbol{\beta}^{*}+\boldsymbol{\epsilon}) = \boldsymbol{\beta}^{*} + \left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{\epsilon}$$记$$\boldsymbol{x}^{\boldsymbol{\dagger}}=\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{\boldsymbol{T}}$$则$$\mathbb{E}\left[\widehat{\boldsymbol{\beta}} \right] = \mathbb{E}\left[\boldsymbol{\beta}^{*}  + \boldsymbol{x}^{\boldsymbol{\dagger}}\boldsymbol{\epsilon} \right] = \boldsymbol{\beta}^{*} +  \mathbb{E}\left[\boldsymbol{x}^{\boldsymbol{\dagger}}\boldsymbol{\epsilon} \right]$$其中$\boldsymbol{x}^{\boldsymbol{\dagger}}$是常量，$\boldsymbol{\epsilon}\sim \mathcal{N}(\boldsymbol{0},\sigma^2 I)$，由高斯分布的线性性，可得$$\mathbb{E}\left[\widehat{\boldsymbol{\beta}} \right] = \boldsymbol{\beta}^{*} $$也即$$\operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{0}\right)\right)=0$$进而$$\frac{1}{n} \sum_{i=1}^{n} \operatorname{Bias}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right)=0$$

2. 现在我们考虑回归输出的方差。试证明:$$
\frac{1}{n} \sum_{i=1}^{n} \operatorname{Var}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right)=\frac{\sigma^{2} p}{n}
$$

> 答：
> [参考资料1](https://www.wikiwand.com/zh-hans/%E8%B7%A1)
> [参考资料2](http://www.stat.cmu.edu/~larry/=stat401/lecture-13.pdf)$$
\frac{1}{n} \sum_{i=1}^{n} \operatorname{Var}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right) = \frac{1}{n} \sum_{i=1}^{n}  \mathbb{E}\left[\left(\hat{f}\left(\boldsymbol{x}_{i}\right) - \mathbb{E}\left[\hat{f}\left(\boldsymbol{x}_{i}\right)\right]\right)^2\right]
$$而由第一问知$$\hat{f}\left(\boldsymbol{x}_{i}\right)=\boldsymbol{x}_{i}^{T} \widehat{\boldsymbol{\beta}}$$$$\mathbb{E}\left[\hat{f}\left(\boldsymbol{x}_{i}\right)\right] = \mathbb{E}\left[\boldsymbol{x}_{i}^{T} \widehat{\boldsymbol{\beta}}\right] = \boldsymbol{x}_{i}^{T}  \mathbb{E}\left[\widehat{\boldsymbol{\beta}}\right] = \boldsymbol{x}_{i}^{T} \boldsymbol{\beta}^{*} $$因此$$\frac{1}{n} \sum_{i=1}^{n} \operatorname{Var}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right) = \frac{1}{n} \sum_{i=1}^{n}  \mathbb{E}\left[\left(\boldsymbol{x}_{i}^{T} \left(\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}^{*}\right)\right)^2\right]$$利用第一问$$\widehat{\boldsymbol{\beta}}  = \boldsymbol{\beta}^{*} + \boldsymbol{x}^{\boldsymbol{\dagger}} \boldsymbol{\epsilon}$$可得$$\begin{aligned}\frac{1}{n} \sum_{i=1}^{n} \operatorname{Var}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right) &= \frac{1}{n} \sum_{i=1}^{n}  \mathbb{E}\left[\left(\boldsymbol{x}_{i}^{T} \boldsymbol{x}^{\boldsymbol{\dagger}} \boldsymbol{\epsilon}\right)^2\right] \\ &= \frac{1}{n} \sum_{i=1}^{n}  \mathbb{E}\left[\left(\boldsymbol{x}_{i}^{T} \boldsymbol{x}^{\boldsymbol{\dagger}} \boldsymbol{\epsilon}\right)\left(\boldsymbol{x}_{i}^{T} \boldsymbol{x}^{\boldsymbol{\dagger}} \boldsymbol{\epsilon}\right)^{T}\right] \\ &= \frac{1}{n} \sum_{i=1}^{n}  \mathbb{E}\left[ \boldsymbol{x}_{i}^{T}\boldsymbol{x}^{\boldsymbol{\dagger}} \boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}{\boldsymbol{x}^{\boldsymbol{\dagger}}}^{T} \boldsymbol{x}_{i} \right] \\&= \frac{1}{n} \operatorname{tr}\left(\mathbb{E}\left[ \boldsymbol{x}\boldsymbol{x}^{\boldsymbol{\dagger}} \boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}{\boldsymbol{x}^{\boldsymbol{\dagger}}}^{T} \boldsymbol{x}^{T} \right]\right)\end{aligned}$$
$\boldsymbol{x}\boldsymbol{x}^{\boldsymbol{\dagger}}$，$\boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}$，${\boldsymbol{x}^{\boldsymbol{\dagger}}}^{T} \boldsymbol{x}^{T}$均为$n\times n$对称矩阵，由矩阵迹的循环性质，可得$$\begin{aligned}\frac{1}{n} \sum_{i=1}^{n} \operatorname{Var}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right) &= \frac{1}{n} \operatorname{tr}\left(\mathbb{E}\left[ \boldsymbol{x}\boldsymbol{x}^{\boldsymbol{\dagger}} {\boldsymbol{x}^{\boldsymbol{\dagger}}}^{T} \boldsymbol{x}^{T} \boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\right]\right)\\&=\frac{1}{n} \operatorname{tr}\left(\mathbb{E}\left[ \boldsymbol{x}\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{\boldsymbol{T}}\boldsymbol{x}\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{T} \boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\right]\right)\\&=\frac{1}{n} \operatorname{tr}\left(\mathbb{E}\left[\boldsymbol{x}\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{T} \boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\right]\right)\end{aligned}$$记帽矩阵$$\mathbf{H} = \boldsymbol{x}\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{T} $$则矩阵迹的循环性质有$$\operatorname{tr}\left(\mathbf{H}\right) = \operatorname{tr}\left(\boldsymbol{x}\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}} \boldsymbol{x}^{T} \right) = \operatorname{tr}\left( \boldsymbol{x}^{T}  \boldsymbol{x}\left(\boldsymbol{x}^{\boldsymbol{T}} \boldsymbol{x}\right)^{-\mathbf{1}}\right) =  \operatorname{tr}\left( \mathbf{I}_{(p\times p)}\right) = p$$因此$$\begin{aligned}\frac{1}{n} \sum_{i=1}^{n} \operatorname{Var}\left(\hat{f}\left(\boldsymbol{x}_{i}\right)\right) &= \frac{1}{n} \operatorname{tr}\left(\mathbb{E}\left[\mathbf{H} \boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\right]\right)\\ &=\frac{1}{n} \operatorname{tr}\left(\mathbf{H}\mathbb{E}\left[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\right]\right) \\ &=\frac{1}{n} \operatorname{tr}\left(\mathbf{H}\left(\operatorname{Var}(\boldsymbol{\epsilon}) + \mathbb{E}\left[\boldsymbol{\epsilon}\right]\mathbb{E}\left[\boldsymbol{\epsilon}\right]^{T}\right)\right)\\ &=\frac{1}{n} \operatorname{tr}\left(\mathbf{H}\operatorname{Var}(\boldsymbol{\epsilon}) \right) \\ &=\frac{1}{n} \operatorname{tr}\left(\mathbf{H} \sigma^2 \mathbf{I} \right) \\ &=\frac{\sigma^2}{n} \operatorname{tr}\left(\mathbf{H}\right)  \\&=\frac{\sigma^{2}p}{n}\end{aligned}$$

3. 根据课上所讲均方误差与偏差和方差的关系，计算模型的期望测试误差$\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}^{\prime}-\hat{f}\left(\boldsymbol{x}_{i}\right)\right)^{2}\right]$，其中，$y_{1}^{\prime}, \ldots, y_{n}^{\prime}$是独立的测试集。

> 由$$
\mathbb{E}\left[\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2}\right]=\operatorname{Var}\left(\hat{f}\left(x_{0}\right)\right)+\left[\operatorname{Bias}\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}+\operatorname{Var}(\epsilon)
$$知，期望测试误差$$\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}^{\prime}-\hat{f}\left(\boldsymbol{x}_{i}\right)\right)^{2}\right] = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[\left(y_{i}^{\prime}-\hat{f}\left(\boldsymbol{x}_{i}\right)\right)^{2}\right] = \frac{1}{n} \sum_{i=1}^{n} \left[\operatorname{Var}\left(\hat{f}\left(x_{i}\right)\right)+\left[\operatorname{Bias}\left(\hat{f}\left(x_{i}\right)\right)\right]^{2}+\operatorname{Var}(\epsilon)\right]$$由第一问和第二问的结论知$$\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}^{\prime}-\hat{f}\left(\boldsymbol{x}_{i}\right)\right)^{2}\right] = \frac{\sigma^{2} p}{n} + 0 + \sigma^{2} = (1+ \frac{p}{n})\sigma^{2}$$

## 10.2 计算机小实验 1:线性回归、岭回归和 LASSO 回归 

请编写代码生成以下仿真数据，探索线性回归、岭回归和 LASSO 回归模型对共线性问题的表现。
$$
\begin{array}{c}
y=3 x_{1}+2+\varepsilon_{1}, x_{1}=1, \ldots, 20 \\
x_{2}=0.05 x_{1}+\varepsilon_{2}
\end{array}
$$$$
\varepsilon_{1} \in \mathrm{N}(0,2), \varepsilon_{2} \in \mathrm{N}(0,0.5)
$$若我们将与$x_1$有强相关关系的噪声$x_2$误认为是一维特征(即输入特征变为了$[x_1, x_2]$)，请同学们尝试使用上述三种模型对$y$进行回归，并回答以下问题。

1. 请给出$x_1, x_2$的相关系数。  
> 答：
```
1 0.5032
2 0.5506
3 0.5797
4 0.2365
5 0.5983
```
> 可见$x_1, x_2$之间有较强的线性相关性

2. 请多次生成数据，观察正则化系数为 1 情况下三种模型拟合参数的稳定性。

> 答：生成五次数据的实验结果如下
```
Linear:
1 coef: [ 3.0797 -0.5201] itcp: 1.5723
2 coef: [2.9775 0.1394] itcp: 2.3713
3 coef: [3.0504 0.1022] itcp: 1.6683
4 coef: [ 3.0393 -0.2393] itcp: 2.3863
5 coef: [ 2.9349 -0.1379] itcp: 3.2367
coef_std: [0.0526 0.2411]
itcp_std: 0.6008

Ridge:
1 coef: [ 3.0711 -0.4811] itcp: 1.6372
2 coef: [2.9723 0.1487] itcp: 2.4225
3 coef: [3.0446 0.1172] itcp: 1.7207
4 coef: [ 3.0338 -0.2087] itcp: 2.4242
5 coef: [ 2.9272 -0.1045] itcp: 3.2921
coef_std: [0.0525 0.2306]
itcp_std: 0.5983

Lasso:
1 coef: [ 2.9967 -0.    ] itcp: 2.1075
2 coef: [2.9591 0.    ] itcp: 2.6223
3 coef: [3.029 0.   ] itcp: 1.9547
4 coef: [ 3.0021 -0.    ] itcp: 2.6235
5 coef: [2.8911 0.    ] itcp: 3.5904
coef_std: [0.0478 0.    ]
itcp_std: 0.5725
```
> 利用标准差来衡量回归系数的稳定性，发现引入共线性之后线性回归拟合的标准差比较大，而引入Ridge和Lasso正则项之后可以在一定程度上提高回归系数的稳定性

3. 针对于岭回归和LASSO，调整正则化系数(调整范围不要过大，0~10之间即可)，你能发现什么。

> ``alpha = 0.01``
```
Ridge:
1 coef: [ 3.0796 -0.5196] itcp: 1.5729
2 coef: [2.9775 0.1395] itcp: 2.3718
3 coef: [3.0504 0.1024] itcp: 1.6688
4 coef: [ 3.0392 -0.2389] itcp: 2.3867
5 coef: [ 2.9349 -0.1375] itcp: 3.2373
coef_std: [0.0526 0.241 ]
itcp_std: 0.6007

Lasso:
1 coef: [ 3.0783 -0.5092] itcp: 1.5801
2 coef: [2.9787 0.1221] itcp: 2.3662
3 coef: [3.0518 0.0824] itcp: 1.6656
4 coef: [ 3.0383 -0.2182] itcp: 2.3826
5 coef: [ 2.9328 -0.1192] itcp: 3.2452
coef_std: [0.0529 0.2282]
itcp_std: 0.602
```
> ``alpha=0.1``
```
Ridge:
1 coef: [ 3.0789 -0.516 ] itcp: 1.5789
2 coef: [2.977  0.1404] itcp: 2.3764
3 coef: [3.0499 0.1039] itcp: 1.6736
4 coef: [ 3.0387 -0.2359] itcp: 2.39
5 coef: [ 2.9341 -0.1343] itcp: 3.2423
coef_std: [0.0526 0.24  ]
itcp_std: 0.6005

Lasso:
1 coef: [ 3.0657 -0.4118] itcp: 1.6496
2 coef: [2.9861 0.    ] itcp: 2.3381
3 coef: [3.056 0.   ] itcp: 1.6705
4 coef: [ 3.03   -0.0287] itcp: 2.3487
5 coef: [ 2.9182 -0.    ] itcp: 3.3062
coef_std: [0.0541 0.1622]
itcp_std: 0.6047
```
> ``alpha=5.0``
```
Ridge:
1 coef: [ 3.0402 -0.3564] itcp: 1.8813
2 coef: [2.9525 0.1736] itcp: 2.6202
3 coef: [3.0232 0.1554] itcp: 1.9223
4 coef: [ 3.0133 -0.1282] itcp: 2.5877
5 coef: [ 2.9005 -0.0112] itcp: 3.5005
coef_std: [0.052  0.1961]
itcp_std: 0.5899

Lasso:
1 coef: [ 2.8764 -0.    ] itcp: 3.3707
2 coef: [2.8388 0.    ] itcp: 3.8855
3 coef: [2.9087 0.    ] itcp: 3.2178
4 coef: [2.8818 0.    ] itcp: 3.8866
5 coef: [2.7708 0.    ] itcp: 4.8535
coef_std: [0.0478 0.    ]
itcp_std: 0.5725
```
> ``alpha=10.0``
```
Ridge:
1 coef: [ 3.0068 -0.2478] itcp: 2.1619
2 coef: [2.9292 0.1903] itcp: 2.8574
3 coef: [2.9988 0.18  ] itcp: 2.1642
4 coef: [ 2.9894 -0.0737] itcp: 2.8039
5 coef: [2.8724 0.0563] itcp: 3.7436
coef_std: [0.0513 0.1652]
itcp_std: 0.5815

Lasso:
1 coef: [2.726 0.   ] itcp: 4.9496
2 coef: [2.6884 0.    ] itcp: 5.4644
3 coef: [2.7583 0.    ] itcp: 4.7968
4 coef: [2.7314 0.    ] itcp: 5.4656
5 coef: [2.6204 0.    ] itcp: 6.4325
coef_std: [0.0478 0.    ]
itcp_std: 0.5725
```
> 答：将以上数据总结成表如下：

$\alpha$ | Ridge_x1 | Lasso_x1 | Ridge_x2 | Lasso_x2 | Ridge_itcp | Lasso_itcp |
|----------|----------|----------|----------|----------|------------|------------|
| 0.01     | 0.0526   | 0.0529   | 0.2410   | 0.2282   | 0.6007     | 0.6020     |
| 0.1      | 0.0526   | 0.0541   | 0.2400   | 0.1622   | 0.6005     | 0.6047     |
| 1.0      | 0.0525   | 0.0478   | 0.2306   | 0.0000   | 0.5983     | 0.5725     |
| 5.0      | 0.0520   | 0.0478   | 0.1961   | 0.0000   | 0.5899     | 0.5725     |
| 10.0     | 0.0513   | 0.0478   | 0.1652   | 0.0000   | 0.5815     | 0.5725     |

> 从以上数据中可以看出，随着正则项系数的增大，Ridge和Lasso的回归系数的方差减小，说明回归系数趋于稳定；另外，Lasso正则项回归系数更稳定，对特征的选择性比Ridge正则项更强，具有稀疏选择的作用，在正则系数增大到一定程度时，可以把共线性成分置零；但同时，正则项系数越大，两种正则化的线性回归拟合得到的回归系数的偏差就越大。

> 可见：
> * 共线性的引入会使得线性回归的方差增大（具体地，表现在$(X^{T}X)$的条件数比较大，使得结果高度不稳定）
> * 引入正则项则可以使求逆问题良态化（对于Ridge正则项，用于求逆的矩阵变成$(X^{T}X + \lambda I)$）
> * 降低方差的代价是牺牲了拟合的准确度

## 10.3计算机小实验 2:特征选择  

附件 ``feature_selection_X.txt`` 中给出了 400 个组织样本数据，每一行是一维样本，每一列代表一维特征，``feature_selection_Y.txt`` 中给出了样本对应的标签(1 代表肿瘤组织，0 代表正常组织)。请随机抽取 300 个样本作为训练集，100 个样本作为测试集。使用特征选择算法，挑选出区分不同组织的特征，利用分类器进行分类:

1.  分别用类内类间距离和最大信息系数(互信息的另一种度量方式)的判据选择 1, 5, 10, 20, 50, 100 个特征，用 Logistic 回归进行分类，并比较与不做特征选择时候的模型预测效果；除此之外，请比较两种方法在这些特征个数时挑选出的特征子集有多少特征是相同的
    
> 答：实验结果如下所示
```
Without Feature Selection:
accuracy: 0.86
confusion matrix: [37  9  5 49]

k= 1
Feature Selection Using MIC:
accuracy: 0.92
confusion matrix: [39  7  1 53]
Feature Selection Using Fisher:
accuracy: 0.92
confusion matrix: [39  7  1 53]
Common Feature Index: {47}
Common Feature Counts: 1

k= 5
Feature Selection Using MIC:
accuracy: 0.95
confusion matrix: [45  1  4 50]
Feature Selection Using Fisher:
accuracy: 0.95
confusion matrix: [44  2  3 51]
Common Feature Index: {219, 4, 47}
Common Feature Counts: 3

k= 10
Feature Selection Using MIC:
accuracy: 0.95
confusion matrix: [45  1  4 50]
Feature Selection Using Fisher:
accuracy: 0.92
confusion matrix: [44  2  6 48]
Common Feature Index: {219, 4, 47}
Common Feature Counts: 3

k= 20
Feature Selection Using MIC:
accuracy: 0.95
confusion matrix: [45  1  4 50]
Feature Selection Using Fisher:
accuracy: 0.91
confusion matrix: [44  2  7 47]
Common Feature Index: {4, 38, 47, 15, 219}
Common Feature Counts: 5

k= 50
Feature Selection Using MIC:
accuracy: 0.88
confusion matrix: [42  4  8 46]
Feature Selection Using Fisher:
accuracy: 0.83
confusion matrix: [44  2 15 39]
Common Feature Index: {291, 4, 38, 39, 15, 47, 219}
Common Feature Counts: 7

k= 100
Feature Selection Using MIC:
accuracy: 0.91
confusion matrix: [45  1  8 46]
Feature Selection Using Fisher:
accuracy: 0.9
confusion matrix: [43  3  7 47]
Common Feature Index: {32, 34, 291, 4, 196, 38, 39, 106, 250, 206, 15, 47, 210, 211, 26, 219}
Common Feature Counts: 16
```
> 结论：
> * 与不做特征选择时相比，经过特征选择后的模型预测效果均有显著提升；
> * 利用MIC判据的模型在``k=5-20``时准确率达到最优，之后下降；利用Fisher判据的模型在`k=5`时达到最优；
>* 总体来看，MIC判据得到的准确率比Fisher判据更高；
>* MIC判据和Fisher判据可以得到部分相同的特征（k=1时得到的特征相同）；另外随着允许的特征数增加，重合的特征也逐渐增多，但重合特征所占的比例渐趋下降，最终稳定在15%左右

2.  请简述前向算法的流程，使用前向算法进行特征选择,并使用 Logistic 回归作为分类器。并比较与(1)中选出特征的异同。

>SFS流程：
>1. 初始化空特征集$\mathcal{M}_{0}$
>2. 对于$k=0, \ldots, p-1$: 
	(a) 考虑在$\mathcal{M}_{k}$基础上增添一个特征的所有$(p-k)$个集合
	(b) 在$(p-k)$个集合中选择根据残差平方和（RSS）或者$R^{2}$定义的最优的集合$\mathcal{M}_{k+1}$
> 3. 在$\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}$中，根据交叉验证预测误差（$C_{p}(\mathrm{AIC}), \mathrm{BIC}$或者调整$R^2$），选择最优的模型
>
> 实验结果如下所示：
```
k= 1
Common Feature Index for MIC: {47}
Common Feature Counts for MIC: 1
Common Feature Index for Fisher: {47}
Common Feature Counts for Fisher: 1
Common Feature Index for All: {47}
Common Feature Counts for All: 1

k= 5
Common Feature Index for MIC: {916, 219, 4, 47}
Common Feature Counts for MIC: 4
Common Feature Index for Fisher: {219, 4, 47}
Common Feature Counts for Fisher: 3
Common Feature Index for All: {219, 4, 47}
Common Feature Counts for All: 3

k= 10
Common Feature Index for MIC: {916, 219, 4, 47}
Common Feature Counts for MIC: 4
Common Feature Index for Fisher: {219, 4, 47}
Common Feature Counts for Fisher: 3
Common Feature Index for All: {219, 4, 47}
Common Feature Counts for All: 3

k= 20
Common Feature Index for MIC: {291, 4, 47, 916, 219}
Common Feature Counts for MIC: 5
Common Feature Index for Fisher: {219, 4, 46, 47}
Common Feature Counts for Fisher: 4
Common Feature Index for All: {219, 4, 47}
Common Feature Counts for All: 3

k= 50
Common Feature Index for MIC: {291, 4, 356, 39, 15, 47, 210, 916, 219}
Common Feature Counts for MIC: 9
Common Feature Index for Fisher: {291, 4, 37, 39, 46, 15, 47, 115, 53, 26, 219}
Common Feature Counts for Fisher: 11
Common Feature Index for All: {291, 4, 39, 47, 15, 219}
Common Feature Counts for All: 6

k= 100
Common Feature Index for MIC: {35, 4, 291, 99, 39, 219, 15, 47, 110, 210, 916, 118, 26, 411}
Common Feature Counts for MIC: 14
Common Feature Index for Fisher: {224, 291, 4, 37, 39, 46, 15, 47, 210, 115, 53, 26, 219}
Common Feature Counts for Fisher: 13
Common Feature Index for All: {291, 4, 39, 47, 15, 210, 26, 219}
Common Feature Counts for All: 8
```
> 实验结果列表如下：

k   | MIC & Fisher | MIC & SFS | Fisher & SFS | MIC & Fisher & SFS
|-----|--------------|-----------|--------------|--------------------|
| 1   | 1            | 1         | 1            | 1                  |
| 5   | 3            | 4         | 3            | 3                  |
| 10  | 3            | 4         | 3            | 3                  |
| 20  | 5            | 5         | 4            | 3                  |
| 50  | 7            | 9         | 11            | 6                  |
| 100 | 16           | 14         | 13           | 8                  |
> 从实验结果可以看出：
> * SFS算法和之前（1）中采用的Filter方法均有一定重合
> * 随着允许的特征个数增多，重合的特征数逐渐增多
> * SFS随着允许的特征个数增多，重合特征的比例逐渐降低

3.  决策树算法在学习过程中会自动选择特征。请使用决策树对数据进行分类， 并观察比较决策树中用到的特征与(1)和(2)中选出的特征的重合程度。

> 答：实验结果如下所示：
```
accuracy: 0.93
confusion matrix: [44  2  5 49]

k= 1
Common Feature Index for MIC: {47}
Common Feature Counts for MIC: 1
Common Feature Index for Fisher: {47}
Common Feature Counts for Fisher: 1
Common Feature Index for SFS: {47}
Common Feature Counts for SFS: 1
Common Feature Index for All: {47}
Common Feature Counts for All: 1

k= 5
Common Feature Index for MIC: {916, 4, 47}
Common Feature Counts for MIC: 3
Common Feature Index for Fisher: {4, 47}
Common Feature Counts for Fisher: 2
Common Feature Index for SFS: {916, 4, 47}
Common Feature Counts for SFS: 3
Common Feature Index for All: {4, 47}
Common Feature Counts for All: 2

k= 10
Common Feature Index for MIC: {916, 4, 47}
Common Feature Counts for MIC: 3
Common Feature Index for Fisher: {4, 47}
Common Feature Counts for Fisher: 2
Common Feature Index for SFS: {916, 4, 47}
Common Feature Counts for SFS: 3
Common Feature Index for All: {4, 47}
Common Feature Counts for All: 2

k= 20
Common Feature Index for MIC: {916, 4, 47}
Common Feature Counts for MIC: 3
Common Feature Index for Fisher: {4, 47}
Common Feature Counts for Fisher: 2
Common Feature Index for SFS: {916, 4, 47}
Common Feature Counts for SFS: 3
Common Feature Index for All: {4, 47}
Common Feature Counts for All: 2

k= 50
Common Feature Index for MIC: {4, 326, 333, 334, 47, 369, 916}
Common Feature Counts for MIC: 7
Common Feature Index for Fisher: {4, 47}
Common Feature Counts for Fisher: 2
Common Feature Index for SFS: {322, 4, 359, 47, 656, 916}
Common Feature Counts for SFS: 6
Common Feature Index for All: {4, 47}
Common Feature Counts for All: 2

k= 100
Common Feature Index for MIC: {4, 356, 326, 333, 334, 47, 369, 916, 254}
Common Feature Counts for MIC: 9
Common Feature Index for Fisher: {256, 4, 263, 264, 269, 47, 271, 275, 280, 253}
Common Feature Counts for Fisher: 10
Common Feature Index for SFS: {322, 4, 358, 359, 47, 656, 916, 276, 255}
Common Feature Counts for SFS: 9
Common Feature Index for All: {4, 47}
Common Feature Counts for All: 2
```
> 整理成下表：

k   | MIC & Fisher | MIC & SFS | Fisher & SFS | MIC & Fisher & SFS | MIC & DeT | Fisher & DeT | SFS & DeT | MIC & Fisher & SFS & DeT |
|-----|--------------|-----------|--------------|--------------------|-----------|--------------|-----------|--------------------------|
| 1   | 1            | 1         | 1            | 1                  | 1         | 1            | 1         | 1                        |
| 5   | 3            | 4         | 3            | 3                  | 3         | 2            | 3         | 2                        |
| 10  | 3            | 4         | 3            | 3                  | 3         | 2            | 3         | 2                        |
| 20  | 5            | 5         | 4            | 3                  | 3         | 2            | 3         | 2                        |
| 50  | 7            | 9         | 11           | 6                  | 7         | 2            | 6         | 2                        |
| 100 | 16           | 14        | 13           | 8                  | 9         | 10           | 9         | 2                        |
> 从实验结果可以看出
> * 决策树算法和之前（1）、（2）中采用的方法均有一定重合
> * 随着允许的特征个数增多，重合的特征数也逐渐增多
> * 随着允许的特征个数增多，重合特征的比例逐渐降低
> * 总体而言，决策树选择的特征的重合程度不及前三者
> * 所有的算法都关注到了4、47两个特征
> * <font color="red"> 若查看决策树算法的特征重要程度，可以发现绝大多数均为0</font>
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTIwNTk1MjQ1Nl19
-->